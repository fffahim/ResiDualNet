{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as mlt\n",
    "import seaborn as sp\n",
    "from torch.autograd import Variable\n",
    "from torch import autograd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "from datetime import timedelta\n",
    "import torch.autograd.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHelper():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def gen_label(self, size, is_real=True, noise_ratio=0.1):\n",
    "        if is_real:\n",
    "            label = torch.ones(size, self.config.lag_size, 1)\n",
    "        else:\n",
    "            label = torch.zeros(size, self.config.lag_size, 1)\n",
    "        return label.to(self.config.device)\n",
    "\n",
    "    def gen_z_input(self, batch_size, step, dset, dset_mask):\n",
    "        return [dset[step * batch_size: (step + 1) * batch_size], dset_mask[step * batch_size: (step + 1) * batch_size]]\n",
    "\n",
    "\n",
    "    def gen_fake_batch(self, generator, batch_size, step, dset, dset_mask):\n",
    "        z = self.gen_z_input(batch_size, step, dset, dset_mask)\n",
    "        fake_dset = generator.predict(z)\n",
    "        fake_label = self.gen_label(batch_size, is_real=False)\n",
    "        return fake_dset, fake_label\n",
    "\n",
    "\n",
    "    def gen_real_batch(self, batch_size, step, dset):\n",
    "        real_dset = dset[step * batch_size: (step + 1) * batch_size]\n",
    "        real_label = self.gen_label(batch_size, is_real=True)\n",
    "        return real_dset, real_label\n",
    "\n",
    "    def gen_random_batch(self, batch_size, step, dset):\n",
    "        random_noise = dset[step * batch_size: (step + 1) * batch_size]\n",
    "        return random_noise\n",
    "        \n",
    "    def calculate_gradient_penalty(self, discriminator, real_data, fake_data):\n",
    "        eta = torch.FloatTensor(self.config.batch_size, self.config.lag_size, self.config.input_size).uniform_(0, 1).to(self.config.device)\n",
    "        eta = eta.expand(self.config.batch_size, self.config.lag_size, self.config.input_size)\n",
    "\n",
    "        interpolated = eta * real_data + ((1 - eta) * fake_data)\n",
    "\n",
    "        # define it to calculate gradient\n",
    "        interpolated = Variable(interpolated, requires_grad=True)\n",
    "\n",
    "        # calculate probability of interpolated examples\n",
    "        prob_interpolated = discriminator(interpolated)\n",
    "\n",
    "        fake = (torch.ones(prob_interpolated.size()).to(self.config.device))\n",
    "\n",
    "        # calculate gradients of probabilities with respect to examples\n",
    "        gradients = autograd.grad(outputs=prob_interpolated, inputs=interpolated, grad_outputs=fake, create_graph=True, retain_graph=True)[0]\n",
    "        gradients = gradients.reshape(self.config.batch_size, -1)\n",
    "        gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
    "        grad_penalty = ((gradients_norm - 1) ** 2).mean() * 10\n",
    "        return grad_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
