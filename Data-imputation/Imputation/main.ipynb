{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as mlt\n",
    "import seaborn as sp\n",
    "from torch.autograd import Variable\n",
    "from torch import autograd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "import sys\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, r2_score\n",
    "from scipy.stats import ks_2samp\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = ['Day of week', 'Energy', 'Week Day', 'Year', 'Month', 'Day of month', 'Hour', 'Sum', 'Seasonal', 'Trend', 'Residual']\n",
    "columns = ['Day of week', 'Energy', 'Week Day', 'Year', 'Month', 'Day of month', 'Hour', 'Sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../Pre_process/Data_preprocess.ipynb\n",
    "%run ../Pre_process/Data_postprocess.ipynb\n",
    "%run Model/ResiDualNet.ipynb\n",
    "%run Model/ConvGan.ipynb\n",
    "%run Model/AutoEncoder.ipynb\n",
    "%run Model/Mean_imputation.ipynb\n",
    "%run Model/KNN_imputer.ipynb\n",
    "%run train.ipynb\n",
    "%run wrapper.ipynb\n",
    "%run helper.ipynb\n",
    "%run ../visualize.ipynb\n",
    "%run test.ipynb\n",
    "%run ../validation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre_process_dataset(\"Data/Raw/boulder_2021.csv\", 'boulder')\n",
    "#pre_process_dataset(\"../Data/Raw/boulder_2021.csv\", 'boulder2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config/config_data.json\") as json_data:\n",
    "    data = json.load(json_data, object_hook=lambda d: SimpleNamespace(**d))\n",
    "    parameters_acn = data.acn\n",
    "    parameters_boulder = data.boulder\n",
    "    parameters_paloalto = data.paloalto\n",
    "    parameters_sap = data.sap\n",
    "    parameters_perth = data.perth\n",
    "    parameters_dundee = data.dundee\n",
    "    parameters_caltech = data.caltech\n",
    "    parameters_jpl = data.jpl\n",
    "    parameters_office = data.office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config/config_model.json\") as json_data:\n",
    "    data = json.load(json_data, object_hook=lambda d: SimpleNamespace(**d))\n",
    "    parameters_seq2seq = data.impute_40.seq2seq\n",
    "    parameters_seq2seq.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_model = parameters_seq2seq\n",
    "parameter_data = parameters_office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ratio = 0.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/Processed/office_data_with_zero.csv\")\n",
    "df['Hour'] = pd.to_datetime(df['Start']).dt.hour\n",
    "df['Sum'] = df.groupby(pd.to_datetime(df['Start']).dt.date)['Energy'].cumsum()\n",
    "df = df.copy().loc[(df['Start'] >= \"2018-10-01 00:00:00\") & (df['Start'] <= \"2020-02-29 23:00:00\")].reset_index(drop=True)\n",
    "df['Start'] = pd.to_datetime(df['Start'])\n",
    "df.set_index('Start', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = seasonal_decompose(df['Energy'], model='additive', extrapolate_trend='freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Seasonal'] = result.seasonal\n",
    "df['Trend'] = result.trend\n",
    "df['Residual'] = result.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_column = df.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "df = scaler.fit_transform(df.iloc[:, 1:])\n",
    "df = pd.DataFrame(df, columns=columns)\n",
    "df = pd.concat([first_column, df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size = df.shape[1]\n",
    "# hidden_size = input_size * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.copy().loc[(df['Start'] >= parameter_data.train.start) & (df['Start'] <= parameter_data.train.end)].reset_index(drop=True)\n",
    "df_train.drop(columns=['Start'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.copy().loc[(df['Start'] >= parameter_data.test.start) & (df['Start'] <= parameter_data.test.end)].reset_index(drop=True)\n",
    "df_test.drop(columns=['Start'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = df.copy().loc[(df['Start'] >= parameter_data.train.start)].reset_index(drop=True)\n",
    "# train_ratio = df.copy().loc[(df['Start'] >= parameter_data.train.start) & (df['Start'] <= parameter_data.train.end)].reset_index(drop=True).shape[0]\n",
    "df.drop(columns=['Start'], inplace=True)\n",
    "# train_ratio = round(train_ratio / df.shape[0], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler(feature_range=(0,1))\n",
    "# df = scaler.fit_transform(df.iloc[:, 1:])\n",
    "# df = pd.DataFrame(df, columns=columns)\n",
    "#df = pd.DataFrame(df, columns=['Energy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "ratio = round(missing_ratio * len(df_train))\n",
    "random_row_indices_train = np.random.choice(df_train.index, size=ratio, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "ratio = round(missing_ratio * len(df_test))\n",
    "random_row_indices_test = np.random.choice(df_test.index, size=ratio, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_row_indices_test.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_train, real_train, mask_train = get_train_test_dataset_imputation(df_train, 0, parameter_model.lag_size, random_row_indices_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_test, real_test, mask_test = get_train_test_dataset_imputation(df_test, 0, parameter_model.lag_size, random_row_indices_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_per_epoch = len(missing_train) // batch_size\n",
    "\n",
    "# Converting to tensor\n",
    "real_train = torch.from_numpy(real_train).float().to(device)\n",
    "missing_train = torch.from_numpy(missing_train).float().to(device)\n",
    "mask_train = torch.from_numpy(mask_train).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator(input_size, hidden_size, 1).to(device)\n",
    "generator = Generator(input_size, hidden_size, input_size).to(device)\n",
    "optimizer_discriminator = torch.optim.SGD(discriminator.parameters(), lr = learning_rate)\n",
    "optimizer_generator = torch.optim.Adam(generator.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function_MSE = nn.MSELoss()\n",
    "loss_function = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dataset, gen_dataset, errors_generator, errors_discriminator = train_Gan(generator, discriminator, optimizer_discriminator, optimizer_generator, loss_function, loss_function_MSE, real_train, missing_train, mask_train, step_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlt.suptitle('Loss')\n",
    "mlt.plot(errors_discriminator, label='d_loss')\n",
    "mlt.plot(errors_generator, label='g_loss')\n",
    "mlt.legend()\n",
    "#mlt.savefig('foo1.png')\n",
    "mlt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_gen_dataset = gen_dataset\n",
    "tr = real_dataset[:200, -1, 1].view(-1)\n",
    "te = gen_dataset[:200, -1, 1].view(-1)\n",
    "mlt.figure(figsize=(20, 6))\n",
    "mlt.suptitle('Gan prediction on training dataset')\n",
    "plt.ylabel('Energy Consumption in Kwh')\n",
    "mlt.plot(tr.detach().cpu().numpy(), label='real')\n",
    "mlt.plot(te.detach().cpu().numpy(), label='gen')\n",
    "mlt.legend()\n",
    "#mlt.savefig('./Results/train_res_impute.png')\n",
    "mlt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_statistic, p_value = ks_2samp(tr.detach().cpu().numpy(), te.detach().cpu().numpy())\n",
    "\n",
    "# Print the results\n",
    "print(\"KS Statistic:\", ks_statistic)\n",
    "print(\"P-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0  # Mean of the distribution\n",
    "std_dev = 1  # Standard deviation of the distribution\n",
    "\n",
    "# Generate random data from a normal distribution\n",
    "random_data = np.random.normal(loc=mean, scale=std_dev, size=(batch_size, lag_size, 1))\n",
    "#random_data = np.clip(random_data, 0, 1)\n",
    "random_data = torch.tensor(random_data,dtype=torch.float32, requires_grad=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_test = torch.from_numpy(real_test).float().to(device)\n",
    "missing_test = torch.from_numpy(missing_test).float().to(device)\n",
    "mask_test = torch.from_numpy(mask_test).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data, real_label = gen_real_batch(real_test.shape[0], 0, real_test)\n",
    "z_input, mask_input = gen_z_input(batch_size, 0, missing_test, mask_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test = z_input + random_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = generator(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr2 = z_input[:, -1, 0].view(-1)\n",
    "te2 = ttt[:, -1, 0].view(-1)\n",
    "# for i in range(len(te)):\n",
    "#     te[i] = max(0, te[i])\n",
    "#     if te[i] > 0:\n",
    "#         te[i] = te[i] + 0.2\n",
    "mlt.figure(figsize=(20, 6))\n",
    "mlt.suptitle('Gan prediction on testing dataset')\n",
    "mlt.plot(tr2.detach().cpu().numpy(), label='real')\n",
    "mlt.plot(te2.detach().cpu().numpy(), label='gen')\n",
    "plt.ylabel('Energy Consumption in Kwh')\n",
    "mlt.legend()\n",
    "#mlt.savefig('./Results/test_res_inpute.png')\n",
    "mlt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = mean_squared_error(tr2.detach().cpu().numpy(), te2.detach().cpu().numpy(), squared=False)\n",
    "print(f'RMSE:{RMSE}')\n",
    "\n",
    "mae = mean_absolute_error(tr2.detach().cpu().numpy(), te2.detach().cpu().numpy())\n",
    "print(\"MAE:\", mae)\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(tr2.detach().cpu().numpy(), te2.detach().cpu().numpy())\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = mean_squared_error(tr.detach().cpu().numpy(), te.detach().cpu().numpy(), squared=False)\n",
    "print(f'Test dataset RMSE:{RMSE}')\n",
    "\n",
    "mae = mean_absolute_error(tr.detach().cpu().numpy(), te.detach().cpu().numpy())\n",
    "print(\"MAE:\", mae)\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(tr.detach().cpu().numpy(), te.detach().cpu().numpy())\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_statistic, p_value = ks_2samp(tr2.detach().cpu().numpy(), te2.detach().cpu().numpy())\n",
    "\n",
    "# Print the results\n",
    "print(\"KS Statistic:\", ks_statistic)\n",
    "print(\"P-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05  \n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: The samples come from different distributions.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResiDualNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainning------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResiDualNet(parameter_model).to(device)\n",
    "#model = Seq2SeqAttention(input_size, hidden_size, input_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = parameter_model.learning_rate, weight_decay = 0.005)\n",
    "loss_function_seq2seq = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = ModelTrain(parameter_model)\n",
    "helper = ModelHelper(parameter_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dataset, gen_dataset, errors_generator, mask_data = wrapper.train_Seq2Seq(model, optimizer, loss_function_seq2seq, real_train, missing_train, mask_train, step_per_epoch, helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlt.suptitle('Loss')\n",
    "mlt.plot(errors_generator, label='d_loss')\n",
    "mlt.legend()\n",
    "#mlt.savefig('foo1.png')\n",
    "mlt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_imputation_results_two(real_dataset.detach().cpu().numpy(), gen_dataset.detach().cpu().numpy(), mask_data.detach().cpu().numpy(), 300, 450, 'Results/test21.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_full_dataset(real_dataset.detach().cpu().numpy(), gen_dataset.detach().cpu().numpy(), 000, 30000, 'Results/test1.png', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dataset_train_seq2seq = pd.DataFrame(real_dataset.detach().cpu().numpy(), columns=columns)\n",
    "real_dataset_train_seq2seq = scaler.inverse_transform(real_dataset_train_seq2seq)\n",
    "real_dataset_train_seq2seq = torch.tensor(real_dataset_train_seq2seq)\n",
    "#real_dataset_train_seq2seq = pd.DataFrame(real_dataset_train_seq2seq.detach().cpu().numpy(), columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_dataset_train_seq2seq = pd.DataFrame(gen_dataset.detach().cpu().numpy(), columns=columns)\n",
    "gen_dataset_train_seq2seq = scaler.inverse_transform(gen_dataset_train_seq2seq)\n",
    "gen_dataset_train_seq2seq = torch.tensor(gen_dataset_train_seq2seq)\n",
    "#gen_dataset_train_seq2seq = pd.DataFrame(gen_dataset_train_seq2seq.detach().cpu().numpy(), columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_test = real_test[: -(real_test.shape[0] % parameter_model.batch_size)]\n",
    "missing_test = missing_test[: -(missing_test.shape[0] % parameter_model.batch_size)]\n",
    "mask_test = mask_test[: -(mask_test.shape[0] % parameter_model.batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_per_epoch = len(missing_test) // parameter_model.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr = df['Energy']\n",
    "# mlt.figure(figsize=(20, 6))\n",
    "# mlt.suptitle('Gan prediction on test dataset')\n",
    "# mlt.ylabel('Energy Consumption in Kwh')\n",
    "# mlt.plot(tr, label='real')\n",
    "# mlt.legend()\n",
    "# mlt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_test = torch.from_numpy(real_test).float().to(device)\n",
    "missing_test = torch.from_numpy(missing_test).float().to(device)\n",
    "mask_test = torch.from_numpy(mask_test).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper_test = ModelTest(parameters_seq2seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dataset_test_seq2seq, imputed_dataset, loss, mask_test_result = wrapper_test.test_model(model, real_test, missing_test, mask_test, loss_function_seq2seq, step_per_epoch, helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_dataset_temp = ((1 - mask_test_result) * real_dataset_test_seq2seq) + (mask_test_result * imputed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_dataset_temp2 = scaler.inverse_transform(imputed_dataset_temp.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dataset_test_seq2seq_temp = scaler.inverse_transform(real_dataset_test_seq2seq.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dataset_test_seq2seq_temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'office_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_imputed_data(real_dataset_test_seq2seq, imputed_dataset_temp, \"../Data/Imputed/50_percent/Seq2Seq/\" + dataset_name + \".csv\", scaler, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dataset_test_seq2seq = pd.DataFrame(real_dataset_test_seq2seq.detach().cpu().numpy(), columns=columns)\n",
    "real_dataset_test_seq2seq = scaler.inverse_transform(real_dataset_test_seq2seq)\n",
    "real_dataset_test_seq2seq = torch.tensor(real_dataset_test_seq2seq)\n",
    "#real_dataset_test_seq2seq = pd.DataFrame(real_dataset_test_seq2seq.detach().cpu().numpy(), columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_dataset = pd.DataFrame(imputed_dataset.detach().cpu().numpy(), columns=columns)\n",
    "imputed_dataset = scaler.inverse_transform(imputed_dataset)\n",
    "imputed_dataset = torch.tensor(imputed_dataset)\n",
    "#imputed_dataset = pd.DataFrame(imputed_dataset.detach().cpu().numpy(), columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.nonzero(mask_test_result[:, 1] == 1).view(-1).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_null_at_indices(lst, indices):\n",
    "    return [val if i in indices else np.nan for i, val in enumerate(lst)]\n",
    "\n",
    "# usage\n",
    "imputed_dataset_list = imputed_dataset['Energy'].values.tolist()\n",
    "result_seq2seq = set_null_at_indices(imputed_dataset_list, indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_full_dataset(real_dataset_test_seq2seq.detach().cpu().numpy(), imputed_dataset.detach().cpu().numpy(), 0, 10000, 'Results/test3.png', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlt.suptitle('Loss')\n",
    "#mlt.plot(errors_generator, label='train_loss')\n",
    "mlt.plot(loss[:], label='test_loss')\n",
    "mlt.legend()\n",
    "#mlt.savefig('foo1.png')\n",
    "mlt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.nonzero(mask_test_result[:, 1] == 1).view(-1).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_matrix_forecasting(real_dataset_test_seq2seq[indices, 1], imputed_dataset[indices, 1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_matrix_imputation(real_dataset_test_seq2seq.numpy(), imputed_dataset.numpy(), mask_test_result.detach().cpu().numpy(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_statistic, p_value = ks_2samp(real_dataset_test_seq2seq[:, 1], imputed_dataset[:, 1])\n",
    "\n",
    "# Print the results\n",
    "print(\"KS Statistic:\", ks_statistic)\n",
    "print(\"P-value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv Gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0  # Mean of the distribution\n",
    "std_dev = 1  # Standard deviation of the distribution\n",
    "\n",
    "# Generate random data from a normal distribution\n",
    "random_data = np.random.normal(loc=mean, scale=std_dev, size=(real_train.size()))\n",
    "#random_data = np.clip(random_data, 0, 1)\n",
    "random_data = torch.tensor(random_data,dtype=torch.float32, requires_grad=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCELoss()\n",
    "generator = ConvGenerator(input_size, hidden_size, input_size).to(device)\n",
    "discriminator = ConvDiscriminator(input_size, hidden_size).to(device)\n",
    "optimizer_discriminator = torch.optim.RMSprop(discriminator.parameters(), lr = learning_rate)\n",
    "optimizer_generator = torch.optim.RMSprop(generator.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dataset, gen_dataset, errors_generator, errors_discriminator, mask_results = train_ConvGan(generator, discriminator, optimizer_discriminator, optimizer_generator, loss_function, real_train, missing_train, mask_train, step_per_epoch, random_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_imputation_results(real_dataset, gen_dataset, mask_results,100,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_full_dataset(real_dataset, gen_dataset, 300, 600, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = real_dataset[:, 7, 0]\n",
    "te = gen_dataset[:, 7, 0]\n",
    "ks_statistic, p_value = ks_2samp(tr.detach().cpu().numpy(), te.detach().cpu().numpy())\n",
    "\n",
    "# Print the results\n",
    "print(\"KS Statistic:\", ks_statistic)\n",
    "print(\"P-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlt.suptitle('Loss')\n",
    "mlt.plot(errors_discriminator, label='d_loss')\n",
    "mlt.plot(errors_generator, label='g_loss')\n",
    "mlt.legend()\n",
    "#mlt.savefig('foo1.png')\n",
    "mlt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = mean_squared_error(tr.detach().cpu().numpy(), te.detach().cpu().numpy(), squared=False)\n",
    "print(f'RMSE:{RMSE}')\n",
    "\n",
    "mae = mean_absolute_error(tr.detach().cpu().numpy(), te.detach().cpu().numpy())\n",
    "print(\"MAE:\", mae)\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(tr.detach().cpu().numpy(), te.detach().cpu().numpy())\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_noise_test = torch.tensor(np.random.randn(real_test.shape[0], lag_size, input_size), dtype=torch.float32, requires_grad=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_test = torch.from_numpy(real_test).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_test_conv, real_label_test_conv = gen_real_batch(real_test.shape[0], 0, real_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = generator(real_data_test_conv, random_noise_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_test = real_data_test_conv[:, -1, 0]\n",
    "te_test = test_res[:, -1, 0]\n",
    "ks_statistic_test, p_value_test = ks_2samp(tr_test.detach().cpu().numpy(), te_test.detach().cpu().numpy())\n",
    "\n",
    "# Print the results\n",
    "print(\"KS Statistic:\", ks_statistic_test)\n",
    "print(\"P-value:\", p_value_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_full_dataset(real_data_test_conv, test_res, 300, 600, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = mean_squared_error(tr_test.detach().cpu().numpy(), te_test.detach().cpu().numpy(), squared=False)\n",
    "print(f'RMSE:{RMSE}')\n",
    "\n",
    "mae = mean_absolute_error(tr_test.detach().cpu().numpy(), te_test.detach().cpu().numpy())\n",
    "print(\"MAE:\", mae)\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(tr_test.detach().cpu().numpy(), te_test.detach().cpu().numpy())\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(input_size, hidden_size, 14).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "loss_function_autoencoder = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dataset, gen_dataset, errors_generator, mask_data = train_autoEncoder(model, optimizer, loss_function_autoencoder, real_train, missing_train, mask_train, step_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlt.suptitle('Loss')\n",
    "mlt.plot(errors_generator, label='d_loss')\n",
    "mlt.legend()\n",
    "#mlt.savefig('foo1.png')\n",
    "mlt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_full_dataset(real_dataset[:, -1, :].detach().cpu().numpy(), gen_dataset[:, -1, :].detach().cpu().numpy(), 200, 600, 'Results/test1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ratio = 0.30\n",
    "\n",
    "with open(\"config/config_data.json\") as json_data:\n",
    "    data = json.load(json_data, object_hook=lambda d: SimpleNamespace(**d))\n",
    "    parameters_acn = data.acn\n",
    "    parameters_boulder = data.boulder\n",
    "    parameters_paloalto = data.paloalto\n",
    "    parameters_sap = data.sap\n",
    "    parameters_perth = data.perth\n",
    "    parameters_dundee = data.dundee\n",
    "    parameters_caltech = data.caltech\n",
    "    parameters_jpl = data.jpl\n",
    "    parameters_office = data.office\n",
    "\n",
    "parameter_data = parameters_office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"office\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/Processed/\" + dataset_name + \"_data_with_zero.csv\")\n",
    "df['Hour'] = pd.to_datetime(df['Start']).dt.hour\n",
    "df['Sum'] = df.groupby(pd.to_datetime(df['Start']).dt.date)['Energy'].cumsum()\n",
    "df = df.copy().loc[df['Start'] <= \"2020-12-31 23:00:00\"].reset_index(drop=True)\n",
    "\n",
    "df_test = df.copy().loc[(df['Start'] >= parameter_data.test.start) & (df['Start'] <= parameter_data.test.end)].reset_index(drop=True)\n",
    "\n",
    "# df_test['Start'] = pd.to_datetime(df_test['Start'])\n",
    "# df_test.set_index('Start', inplace=True)\n",
    "# result = seasonal_decompose(df_test['Energy'], model='additive', extrapolate_trend='freq')\n",
    "# df_test['Seasonal'] = result.seasonal\n",
    "# df_test['Trend'] = result.trend\n",
    "# df_test['Residual'] = result.resid\n",
    "# df_test.reset_index(inplace=True)\n",
    "\n",
    "df_test.drop(columns=['Start'], inplace=True)\n",
    "\n",
    "np.random.seed(0)\n",
    "ratio = round(missing_ratio * len(df_test))\n",
    "random_row_indices_test = np.random.choice(df_test.index, size=ratio, replace=False)\n",
    "\n",
    "missing_data = random_index_noise(df_test.copy(), random_row_indices_test)\n",
    "\n",
    "mask = np.isnan(missing_data)\n",
    "mask = mask.replace({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MeanImputation()\n",
    "imputed_data_mean = model(missing_data, 'Energy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data_mean = torch.tensor(imputed_data_mean.values)\n",
    "df_test = torch.tensor(df_test.values)\n",
    "mask = torch.tensor(mask.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data_mean = ((1 - mask) * df_test) + (mask * imputed_data_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_knn = torch.nonzero(mask[:, 1] == 1).view(-1).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_full_dataset(df_test.detach().cpu().numpy(), imputed_data_mean.detach().cpu().numpy(), 000, 500, 'Results/test3.png', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_row_indices_test.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_row_indices_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_matrix_forecasting(df_test[indices_knn, 1], imputed_data_mean[indices_knn, 1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_matrix_imputation(df_test.detach().cpu().numpy(), imputed_data_mean.detach().cpu().numpy(), mask.detach().cpu().numpy(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data = pd.DataFrame(imputed_data_mean, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data['Start'] = imputed_data.apply(lambda row: datetime(int(row['Year']), int(row['Month']), int(row['Day of month'])), axis=1)\n",
    "imputed_data['Sum'] = imputed_data.groupby(pd.to_datetime(imputed_data['Start']).dt.date)['Energy'].cumsum()\n",
    "imputed_data.drop(columns=['Start'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data.to_csv(\"../Data/Imputed/40_percent/Mean/\" + dataset_name + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ratio = 0.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config/config_data.json\") as json_data:\n",
    "    data = json.load(json_data, object_hook=lambda d: SimpleNamespace(**d))\n",
    "    parameters_acn = data.acn\n",
    "    parameters_boulder = data.boulder\n",
    "    parameters_paloalto = data.paloalto\n",
    "    parameters_sap = data.sap\n",
    "    parameters_perth = data.perth\n",
    "    parameters_dundee = data.dundee\n",
    "    parameters_caltech = data.caltech\n",
    "    parameters_jpl = data.jpl\n",
    "    parameters_office = data.office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_data = parameters_office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name  = 'office'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/Processed/\" + dataset_name + \"_data_with_zero.csv\")\n",
    "df['Hour'] = pd.to_datetime(df['Start']).dt.hour\n",
    "df['Sum'] = df.groupby(pd.to_datetime(df['Start']).dt.date)['Energy'].cumsum()\n",
    "df = df.copy().loc[df['Start'] <= \"2020-12-31 23:00:00\"].reset_index(drop=True)\n",
    "\n",
    "df_test = df.copy().loc[(df['Start'] >= parameter_data.test.start) & (df['Start'] <= parameter_data.test.end)].reset_index(drop=True)\n",
    "\n",
    "# df_test['Start'] = pd.to_datetime(df_test['Start'])\n",
    "# df_test.set_index('Start', inplace=True)\n",
    "# result = seasonal_decompose(df_test['Energy'], model='additive', extrapolate_trend='freq')\n",
    "# df_test['Seasonal'] = result.seasonal\n",
    "# df_test['Trend'] = result.trend\n",
    "# df_test['Residual'] = result.resid\n",
    "# df_test.reset_index(inplace=True)\n",
    "\n",
    "df_test.drop(columns=['Start'], inplace=True)\n",
    "\n",
    "np.random.seed(0)\n",
    "ratio = round(missing_ratio * len(df_test))\n",
    "random_row_indices_test = np.random.choice(df_test.index, size=ratio, replace=False)\n",
    "\n",
    "missing_data = random_index_noise(df_test.copy(), random_row_indices_test)\n",
    "\n",
    "mask = np.isnan(missing_data)\n",
    "mask = mask.replace({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_row_indices_test.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_row_indices_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_neighbour = get_optimum_k(df, missing_data)\n",
    "k_neighbour = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KnnImputer()\n",
    "imputed_data = model(missing_data, k_neighbour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data = torch.tensor(imputed_data)\n",
    "df_test = torch.tensor(df_test.values)\n",
    "mask = torch.tensor(mask.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_knn = torch.nonzero(mask[:, 1] == 1).view(-1).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_knn = set_null_at_indices(imputed_data['Energy'].values.tolist(), indices_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_full_dataset(df_test.detach().cpu().numpy(), imputed_data.detach().cpu().numpy(), 000, 100, 'Results/test3.png', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_matrix_forecasting(df_test[indices_knn, 1], imputed_data[indices_knn, 1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_matrix_imputation(df_test.detach().cpu().numpy(), imputed_data.detach().cpu().numpy(), mask.detach().cpu().numpy(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data = pd.DataFrame(imputed_data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data['Start'] = imputed_data.apply(lambda row: datetime(int(row['Year']), int(row['Month']), int(row['Day of month'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data['Sum'] = imputed_data.groupby(pd.to_datetime(imputed_data['Start']).dt.date)['Energy'].cumsum()\n",
    "imputed_data.drop(columns=['Start'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data.to_csv(\"../Data/Imputed/50_percent/Knn/\" + dataset_name + \"_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_dataset = torch.tensor(imputed_dataset)\n",
    "real_dataset_test_seq2seq = torch.tensor(real_dataset_test_seq2seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data = torch.tensor(imputed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_predicted_dataset(real_dataset_test_seq2seq_temp, imputed_dataset_temp2, imputed_data[7:], imputed_data_mean[7:], 640, 750, 'Results/test_palo_last.png', 1, 'Office')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
