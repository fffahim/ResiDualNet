{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.input_size = config.input_size\n",
    "#         self.hidden_size = config.hidden_size\n",
    "#         self.output_size = config.output_size\n",
    "\n",
    "#         self.lstm1 = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, batch_first=True)\n",
    "#         self.batchNorm1 = nn.BatchNorm1d(self.hidden_size)\n",
    "\n",
    "#         self.linear1 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "#         self.batchNorm2 = nn.BatchNorm1d(self.hidden_size)\n",
    "        \n",
    "#         self.lstm2 = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, batch_first=True)\n",
    "#         self.batchNorm3 = nn.BatchNorm1d(self.hidden_size)\n",
    "\n",
    "#         self.linear2 = nn.Linear(self.hidden_size, self.output_size)\n",
    "#         self.activation = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x, z):\n",
    "#         output, (h, c) = self.lstm1(x)\n",
    "#         h = torch.transpose(h, 1, 2)\n",
    "#         h = self.batchNorm1(h)\n",
    "#         h = torch.transpose(h, 1, 2)\n",
    "\n",
    "#         h = self.linear1(h)\n",
    "#         # h = torch.transpose(h, 1, 2)\n",
    "#         # h = self.batchNorm2(h)\n",
    "#         # h = torch.transpose(h, 1, 2)\n",
    "\n",
    "#         output, _ = self.lstm2(z, (h, c))\n",
    "#         output = torch.transpose(output, 1, 2)\n",
    "#         output = self.batchNorm3(output)\n",
    "#         output = torch.transpose(output, 1, 2)\n",
    "\n",
    "#         output = self.linear2(output)\n",
    "        \n",
    "#         return self.activation(output)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_size = config.input_size  # feature size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.latent_dim = config.hidden_size * 2\n",
    "        self.output_size = config.output_size\n",
    "\n",
    "        # Encoder: GRU to map incomplete data to latent space (z)\n",
    "        self.encoder_gru = nn.GRU(input_size=self.input_size, hidden_size=self.hidden_size, batch_first=True)\n",
    "        self.encoder_gru2 = nn.GRU(input_size=self.hidden_size, hidden_size=self.hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(self.hidden_size, self.latent_dim)  # Map hidden state to latent vector (z)\n",
    "\n",
    "        # Decoder: GRU to reconstruct complete data from latent vector (z)\n",
    "        self.decoder_gru = nn.GRU(input_size=self.latent_dim, hidden_size=self.hidden_size, batch_first=True)\n",
    "        self.decoder_gru2 = nn.GRU(input_size=self.hidden_size, hidden_size=self.hidden_size, batch_first=True)\n",
    "        self.fc2 = nn.Linear(self.hidden_size, self.output_size)  # Map hidden state to output\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding: Pass input (batch_size, lag, feature) through encoder GRU\n",
    "        output, h = self.encoder_gru(x)  # h shape: (1, batch_size, hidden_size)\n",
    "        output, h = self.encoder_gru2(output)\n",
    "\n",
    "        # Latent vector z from the hidden state\n",
    "        z = self.fc1(output)  # z shape: (batch_size, latent_dim)\n",
    "\n",
    "\n",
    "        # Pass z through decoder GRU\n",
    "        output, _ = self.decoder_gru(z)  # output shape: (batch_size, lag, hidden_size)\n",
    "        output, _ = self.decoder_gru2(output)\n",
    "\n",
    "        # Map to original output size\n",
    "        output = self.fc2(output)  # output shape: (batch_size, lag, output_size)\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_size = config.input_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.output_size = config.output_size\n",
    "        self.lag_size = config.lag_size\n",
    "\n",
    "        self.gru1 = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, batch_first=True)\n",
    "        self.batchNorm1 = nn.BatchNorm1d(self.hidden_size)\n",
    "\n",
    "        self.linear1 = nn.Linear(self.hidden_size, 1)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.gru1(x)\n",
    "        # output = torch.transpose(output, 1, 2)\n",
    "        # output = self.batchNorm1(output)\n",
    "        # output = torch.transpose(output, 1, 2)\n",
    "\n",
    "        # output = self.flatten(output)\n",
    "        output = self.linear1(output)\n",
    "        \n",
    "        return self.activation(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
