{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as mlt\n",
    "import seaborn as sp\n",
    "from torch.autograd import Variable\n",
    "from torch import autograd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "from datetime import timedelta\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_data_csv = \"Data/Raw/boulder_2021.csv\"\n",
    "resample_frequency = 1\n",
    "split_date = '2021-12-31 23:00:00'\n",
    "processed_data_csv = \"Data/Processed/palo_alto_data_with_zero.csv\"\n",
    "missing_ratio = 0.40\n",
    "lag_size = 7 # lstm - 14, mog - 21, scinet - 9 * 24\n",
    "train_ratio = 0.70\n",
    "learning_rate = 0.0001\n",
    "epoch = 1000\n",
    "hidden_size = 28 #28 for forecasting\n",
    "input_size = 7\n",
    "batch_size = 256\n",
    "train_dis = 5\n",
    "clip = 0.01\n",
    "lambda_term = 10\n",
    "z_input_size = 6\n",
    "future_step = 24\n",
    "stacks = 2\n",
    "levels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Prepare data for imputation model training and evaluation.\")\n",
    "    parser.add_argument(\"--raw_data_csv\",   type=str,   default=\"../Data/Raw/boulder_2021.csv\", help=\"Path to raw data root\")\n",
    "        \n",
    "    parser.add_argument(\"--day_size\",             type=int,   default=48, help=\"Size of a day\")\n",
    "    parser.add_argument(\"--n_days\",               type=int,   default=5, help=\"Number of days\")\n",
    "    parser.add_argument(\"--day_stride\",           type=int,   default=1, help=\"Day stride for sliding window\")\n",
    "    parser.add_argument(\"--resample_frequency\",   type=int,   default=1, help=\"Resample dataset in 1 hour interval\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_dataset(raw_data_csv, dataset_type):\n",
    "    def build_dataset(df):\n",
    "        df['Start'] =  pd.to_datetime(df['Start'])\n",
    "        df['End'] =  pd.to_datetime(df['End'])\n",
    "        return df\n",
    "\n",
    "    def resample_dataset(df, frequency):\n",
    "        hourly_data = []\n",
    "        for _, row in df.iterrows():\n",
    "            plugin_time = row['Start']\n",
    "            plugout_time = row['End']\n",
    "            total_energy = row['Energy']\n",
    "            if 'Charge.Duration' in df.columns:\n",
    "                total_charging_duration = row['Charge.Duration']\n",
    "            else:\n",
    "                total_charging_duration = row['Park.Duration']\n",
    "            #total_charging_duration = row['Charge.Duration']\n",
    "            enery_per_minute = total_energy / total_charging_duration\n",
    "            # Generate hourly rows\n",
    "            while plugin_time < plugout_time and total_charging_duration > 0:\n",
    "                # Round down to the nearest hour\n",
    "                start_time = plugin_time.replace(minute=0, second=0, microsecond=0)\n",
    "\n",
    "                # Add one hour\n",
    "                next_start_time = start_time + timedelta(hours=1)\n",
    "\n",
    "                if next_start_time > plugout_time:\n",
    "                    break\n",
    "                    \n",
    "                time_diff = (next_start_time - plugin_time).total_seconds() / 60\n",
    "                time_diff = min(time_diff, total_charging_duration)\n",
    "                enery_consumption = time_diff * enery_per_minute\n",
    "                total_charging_duration = total_charging_duration - time_diff\n",
    "\n",
    "                hourly_data.append({\n",
    "                    'Start': start_time,\n",
    "                    'End': next_start_time,\n",
    "                    'Day': row['Day'],\n",
    "                    'Energy': enery_consumption,\n",
    "                    'Time Duration': time_diff,\n",
    "                })\n",
    "                \n",
    "                plugin_time = next_start_time\n",
    "            if total_charging_duration == 0:\n",
    "                continue\n",
    "            enery_consumption = total_charging_duration * enery_per_minute\n",
    "            hourly_data.append({\n",
    "                'Start': start_time,\n",
    "                'End': next_start_time,\n",
    "                'Day': row['Day'],\n",
    "                'Energy': enery_consumption,\n",
    "                'Time Duration': plugout_time.minute,\n",
    "            })\n",
    "\n",
    "        # Create a new DataFrame from the hourly data\n",
    "        hourly_df = pd.DataFrame(hourly_data)\n",
    "        return hourly_df\n",
    "    \n",
    "    def aggregate_dataset(df, frequency):\n",
    "        counts = df.groupby('Start').size()\n",
    "        counts = counts.reset_index()\n",
    "        counts = counts.rename(columns = {0: 'stations'})\n",
    "        df = pd.merge(df, counts, on='Start', how='left')\n",
    "\n",
    "        df = df.groupby('Start').agg({\n",
    "            'End': 'first',\n",
    "            'Day': 'first',\n",
    "            'Energy': 'sum',\n",
    "            'stations': 'first'\n",
    "        }).reset_index()\n",
    "\n",
    "        # df['Charging Points'] = df.groupby('Start').size().values\n",
    "        df.set_index('Start', inplace=True)\n",
    "        df = df.resample('1H').asfreq()\n",
    "        df.reset_index(inplace=True)\n",
    "        df.loc[:,'Day'] = df['Start'].dt.dayofweek + 1\n",
    "        df = df.astype({'Day': 'int32'})\n",
    "        df['Week Day'] = (df['Day'] <= 5).astype(int)\n",
    "        df.rename(columns={'Day': 'Day of week'}, inplace=True)\n",
    "        df['Year'] = df['Start'].dt.year\n",
    "        df['Month'] = df['Start'].dt.month\n",
    "        df['Day of month'] = df['Start'].dt.day\n",
    "        df.drop(columns=['End'], inplace=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def interpolate_data(df, type=None):\n",
    "        if type == None:\n",
    "            df['Energy'] = df['Energy'].replace(np.nan, 0)\n",
    "            df['stations'] = df['stations'].replace(np.nan, 0)\n",
    "        elif type == 'linear':\n",
    "            df['Energy'] = df['Energy'].interpolate(method = type, order = 2)\n",
    "        else:\n",
    "            df['Energy'] = df['Energy'].replace(np.nan, 0)\n",
    "            df['Energy'] = df.groupby(df['Start'].dt.date)['Energy'].cumsum()\n",
    "        return df\n",
    "\n",
    "    def create_different_dataset(df, dataset_type):\n",
    "        df_with_zero = interpolate_data(df.copy())\n",
    "        df_with_zero.to_csv('../Data/Processed/' + dataset_type + '_data_with_zero.csv', index=False)\n",
    "\n",
    "        # df_with_imputation = interpolate_data(df.copy(), 'linear');\n",
    "        # df_with_imputation.to_csv('Data/Processed/' + dataset_type + '_data_with_imputation.csv', index=False)\n",
    "\n",
    "        # df_with_cumsum = interpolate_data(df.copy(), 'sum')\n",
    "        # df_with_cumsum.to_csv('Data/Processed/' + dataset_type + '_data_with_cumsum.csv', index=False)\n",
    "\n",
    "    df = pd.read_csv(raw_data_csv)\n",
    "    \n",
    "    df = remove_outliers(df)\n",
    "    df = df.loc[df['Start'] <= split_date]\n",
    "\n",
    "    df = build_dataset(df)\n",
    "    df = resample_dataset(df, resample_frequency)\n",
    "    df = aggregate_dataset(df, resample_frequency)\n",
    "\n",
    "    create_different_dataset(df, dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    column_name = 'Energy'\n",
    "    Q1 = df[column_name].quantile(0.25)\n",
    "    Q3 = df[column_name].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "        \n",
    "    # Define the lower and upper bounds for outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "    # Filter the DataFrame to keep only the non-outliers\n",
    "    df_no_outliers = df[(df[column_name] >= lower_bound) & (df[column_name] <= upper_bound)]\n",
    "    return df_no_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_index_noise(x, random_row_indices, min_size = 2, max_size = 24):\n",
    "    # Get random row indices to set as null\n",
    "    #random_row_indices = np.random.choice(x.index, size=ratio, replace=False)\n",
    "\n",
    "    # width = np.random.randint(min_size, max_size)\n",
    "    # where = np.random.randint(0, len(x) - width)\n",
    "    # x[where: where + width] = np.nan\n",
    "\n",
    "    # Set the values in the first column of the randomly selected rows to null\n",
    "    x.loc[random_row_indices, 'Energy'] = np.nan\n",
    "    # x.loc[random_row_indices, 'Seasonal'] = np.nan\n",
    "    # x.loc[random_row_indices, 'Trend'] = np.nan\n",
    "    # x.loc[random_row_indices, 'Residual'] = np.nan\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_decomposition(dataset):\n",
    "    seasonal_decomposition = seasonal_decompose(dataset['Energy'], model='additive', extrapolate_trend='freq')\n",
    "    dataset['Seasonal'] = seasonal_decomposition.seasonal\n",
    "    dataset['Trend'] = seasonal_decomposition.trend\n",
    "    dataset['Residual'] = seasonal_decomposition.resid\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(real_dataset, missing_data, mask, random_row_indices):\n",
    "    # real_dataset = dataset # Real dataset with no missing data\n",
    "    # missing_data = missing_data # Real dataset with missing data\n",
    "    # mask = np.isnan(missing_data) # Mask for tracking the indexes of the null values\n",
    "\n",
    "    # Convert to nd array\n",
    "    # missing_data = missing_data.replace(np.nan, 0)\n",
    "\n",
    "    real = np.array(real_dataset)\n",
    "    missing = np.array(missing_data)\n",
    "    mask = np.array(mask.replace([False, True],  [0, 1]))\n",
    "    return missing, mask, real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_by_lag_size(dataset, lag):\n",
    "    X_purified = []\n",
    "    for i in range(len(dataset)):\n",
    "        if (len(dataset[i]) == lag):\n",
    "            X_purified.append(dataset[i])\n",
    "    return X_purified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_transform(real, lag, future_step = 0):\n",
    "    X_real = []\n",
    "    for i in range(len(real) - lag - future_step):\n",
    "        lag_data = []\n",
    "        for j in range(i, i+lag):\n",
    "            lag_data.append(real[j])\n",
    "        X_real.append(lag_data)\n",
    "            \n",
    "    X_real = np.stack(filter_data_by_lag_size(X_real, lag))\n",
    "\n",
    "    return X_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, train_ratio):\n",
    "    X_train = X[0: int(len(X) * train_ratio)]\n",
    "    X_test = X[int(len(X) * train_ratio): len(X)]\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_dataset_imputation(df, df_missing_data, df_mask, train_test_ratio, lag_size, random_row_indices):\n",
    "    missing, mask, real = prepare_dataset(df, df_missing_data, df_mask, random_row_indices)\n",
    "    missing = input_transform(missing, lag_size)\n",
    "    mask = input_transform(mask, lag_size)\n",
    "    real = input_transform(real, lag_size)\n",
    "    # missing_train, missing_test = train_test_split(missing, train_test_ratio)\n",
    "    # real_train, real_test = train_test_split(real, train_test_ratio)\n",
    "    # mask_train, mask_test = train_test_split(mask, train_test_ratio)\n",
    "\n",
    "    return missing, real, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forecasting_ground_truth_data(load, window, num):\n",
    "    train_label = []\n",
    "\n",
    "    for i in range(0, len(load) - window - num):\n",
    "        lag_data = []\n",
    "        for j in range(i + window, i + window + num):\n",
    "            lag_data.append(load[j])\n",
    "        train_label.append(lag_data)\n",
    "    train_label = np.stack(train_label)\n",
    "    return train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_dataset_forecasting(df, lag_size, future_step):\n",
    "    real = np.array(df)\n",
    "    data = input_transform(real, lag_size, future_step)\n",
    "    ground_truth = get_forecasting_ground_truth_data(real, lag_size, future_step)\n",
    "    # data_train, data_test = train_test_split(data, train_test_ratio)\n",
    "    # ground_truth_train, ground_truth_test = train_test_split(ground_truth, train_test_ratio)\n",
    "\n",
    "    return data, ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
